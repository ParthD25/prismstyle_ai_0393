{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84ba6c8",
   "metadata": {},
   "source": [
    "# üé® PrismStyle AI - DeepFashion2 Training (H100 Optimized)\n",
    "\n",
    "**Target:** 13-class clothing classification with ‚â•80% validation accuracy\n",
    "\n",
    "**Architecture:** EfficientNetB3 (224√ó224)\n",
    "\n",
    "**Dataset:** DeepFashion2 (191,961 training images)\n",
    "\n",
    "**Hardware:** Google Colab Pro H100 GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Classes:\n",
    "1. short_sleeve_top\n",
    "2. long_sleeve_top\n",
    "3. short_sleeve_outwear\n",
    "4. long_sleeve_outwear\n",
    "5. vest\n",
    "6. sling\n",
    "7. shorts\n",
    "8. trousers\n",
    "9. skirt\n",
    "10. short_sleeve_dress\n",
    "11. long_sleeve_dress\n",
    "12. vest_dress\n",
    "13. sling_dress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300bd27",
   "metadata": {},
   "source": [
    "## üìã Step 1: Environment Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "068af08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name, driver_version, memory.total [MiB]\n",
      "NVIDIA GeForce RTX 5060 Ti, 576.88, 16311 MiB\n",
      "\n",
      "üî• PyTorch version: 2.11.0.dev20260124+cu128\n",
      "üéØ CUDA available: True\n",
      "üöÄ GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "üíæ GPU Memory: 17.10 GB\n"
     ]
    }
   ],
   "source": [
    "# Verify H100 GPU\n",
    "!nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv\n",
    "\n",
    "import torch\n",
    "print(f\"\\nüî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéØ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62275ac8",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b94060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q timm onnx onnxruntime pillow tqdm scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b72c0",
   "metadata": {},
   "source": [
    "## üîê Step 3: Clone GitHub Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c105415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pdave\\Downloads\\prismstyle_ai_0393-main\\PrismStyle_AI\\PrismStyle_AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'PrismStyle_AI'...\n",
      "warning: You appear to have cloned an empty repository.\n"
     ]
    }
   ],
   "source": [
    "# Clone your repository\n",
    "!git clone https://github.com/ParthD25/PrismStyle_AI.git\n",
    "%cd PrismStyle_AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e5815",
   "metadata": {},
   "source": [
    "## üìÇ Step 4: Download & Prepare DeepFashion2 Dataset\n",
    "\n",
    "**Option A:** Use existing downloaded dataset (if you have it in Google Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6187b87e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Mount Google Drive (if dataset is already there)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      3\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Symbolic link to dataset (adjust path if needed)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# !ln -s /content/drive/MyDrive/DeepFashion2/train ./deepfashion2_training/train\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# !ln -s /content/drive/MyDrive/DeepFashion2/validation ./deepfashion2_training/validation\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# !ln -s /content/drive/MyDrive/DeepFashion2/json_for_train ./deepfashion2_training/json_for_train\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# !ln -s /content/drive/MyDrive/DeepFashion2/json_for_validation ./deepfashion2_training/json_for_validation\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive (only if running in Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Not running in Google Colab - skipping drive mount\")\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Symbolic link to dataset (adjust path if needed)\n",
    "# !ln -s /content/drive/MyDrive/DeepFashion2/train ./deepfashion2_training/train\n",
    "# !ln -s /content/drive/MyDrive/DeepFashion2/validation ./deepfashion2_training/validation\n",
    "# !ln -s /content/drive/MyDrive/DeepFashion2/json_for_train ./deepfashion2_training/json_for_train\n",
    "# !ln -s /content/drive/MyDrive/DeepFashion2/json_for_validation ./deepfashion2_training/json_for_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968dee06",
   "metadata": {},
   "source": [
    "**Option B:** Download dataset directly (uncomment if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc488e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd deepfashion2_training\n",
    "# !python download_dataset.py\n",
    "# !python prepare_dataset.py\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c6247",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Step 5: Training Script (H100 Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a95d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# H100 optimization settings\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Training device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b1c16",
   "metadata": {},
   "source": [
    "### Define Class Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07dec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    'short_sleeve_top',\n",
    "    'long_sleeve_top',\n",
    "    'short_sleeve_outwear',\n",
    "    'long_sleeve_outwear',\n",
    "    'vest',\n",
    "    'sling',\n",
    "    'shorts',\n",
    "    'trousers',\n",
    "    'skirt',\n",
    "    'short_sleeve_dress',\n",
    "    'long_sleeve_dress',\n",
    "    'vest_dress',\n",
    "    'sling_dress'\n",
    "]\n",
    "\n",
    "# DeepFashion2 category_id to class index mapping\n",
    "CATEGORY_ID_TO_CLASS = {\n",
    "    1: 0,   # short_sleeve_top\n",
    "    2: 1,   # long_sleeve_top\n",
    "    3: 2,   # short_sleeve_outwear\n",
    "    4: 3,   # long_sleeve_outwear\n",
    "    5: 4,   # vest\n",
    "    6: 5,   # sling\n",
    "    7: 6,   # shorts\n",
    "    8: 7,   # trousers\n",
    "    9: 8,   # skirt\n",
    "    10: 9,  # short_sleeve_dress\n",
    "    11: 10, # long_sleeve_dress\n",
    "    12: 11, # vest_dress\n",
    "    13: 12  # sling_dress\n",
    "}\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "print(f\"üìä Training for {NUM_CLASSES} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954bbe97",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112036cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFashion2Dataset(Dataset):\n",
    "    def __init__(self, image_dir, json_dir, transform=None, mixup_alpha=0.0):\n",
    "        self.image_dir = image_dir\n",
    "        self.json_dir = json_dir\n",
    "        self.transform = transform\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        \n",
    "        # Load all annotations\n",
    "        self.samples = []\n",
    "        json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]\n",
    "        \n",
    "        for json_file in tqdm(json_files, desc=\"Loading annotations\"):\n",
    "            json_path = os.path.join(json_dir, json_file)\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract image name\n",
    "            img_name = data.get('source', json_file.replace('.json', '.jpg'))\n",
    "            img_path = os.path.join(image_dir, img_name)\n",
    "            \n",
    "            # Extract category from first item\n",
    "            if 'item1' in data:\n",
    "                category_id = data['item1'].get('category_id', None)\n",
    "                if category_id in CATEGORY_ID_TO_CLASS:\n",
    "                    class_idx = CATEGORY_ID_TO_CLASS[category_id]\n",
    "                    if os.path.exists(img_path):\n",
    "                        self.samples.append((img_path, class_idx))\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(self.samples)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return a black image on error\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # MixUp augmentation (only during training)\n",
    "        if self.mixup_alpha > 0:\n",
    "            # Return image, label, and index for mixup\n",
    "            return image, label, idx\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f1111",
   "metadata": {},
   "source": [
    "### Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d8253",
   "metadata": {},
   "source": [
    "### Create Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust paths if needed\n",
    "TRAIN_IMAGE_DIR = './deepfashion2_training/train'\n",
    "TRAIN_JSON_DIR = './deepfashion2_training/json_for_train'\n",
    "VAL_IMAGE_DIR = './deepfashion2_training/validation'\n",
    "VAL_JSON_DIR = './deepfashion2_training/json_for_validation'\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DeepFashion2Dataset(\n",
    "    image_dir=TRAIN_IMAGE_DIR,\n",
    "    json_dir=TRAIN_JSON_DIR,\n",
    "    transform=train_transform,\n",
    "    mixup_alpha=0.2\n",
    ")\n",
    "\n",
    "val_dataset = DeepFashion2Dataset(\n",
    "    image_dir=VAL_IMAGE_DIR,\n",
    "    json_dir=VAL_JSON_DIR,\n",
    "    transform=val_transform,\n",
    "    mixup_alpha=0.0\n",
    ")\n",
    "\n",
    "# H100 optimized batch size (can go higher with 80GB VRAM)\n",
    "BATCH_SIZE = 64  # Increase to 128 if memory allows\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"üì¶ Training batches: {len(train_loader)}\")\n",
    "print(f\"üì¶ Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d0455",
   "metadata": {},
   "source": [
    "### MixUp Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Apply MixUp augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"MixUp loss function\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e8cca",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba3744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EfficientNetB3 with pretrained ImageNet weights\n",
    "model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# Compile model for H100 optimization (PyTorch 2.0+)\n",
    "if hasattr(torch, 'compile'):\n",
    "    print(\"üî• Compiling model with torch.compile for H100...\")\n",
    "    model = torch.compile(model, mode='max-autotune')\n",
    "\n",
    "print(f\"‚úÖ Model loaded: EfficientNetB3\")\n",
    "print(f\"üìä Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d036cd7",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MIXUP_ALPHA = 0.2\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler (cosine annealing)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# Mixed precision training (for H100) - Updated API for PyTorch 2.0+\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "print(\"‚úÖ Training configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c532d31",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # ========== Training Phase ==========\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        # Apply MixUp\n",
    "        images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=MIXUP_ALPHA)\n",
    "        \n",
    "        # Mixed precision forward pass - Updated API for PyTorch 2.0+\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Metrics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_preds.extend(predicted.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    \n",
    "    # ========== Validation Phase ==========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
    "    print(f\"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"   Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc*100:.2f}%\")\n",
    "    print(f\"   Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'class_names': CLASS_NAMES\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"   ‚úÖ Best model saved! (Val Acc: {val_acc*100:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"   ‚è≥ Patience: {patience_counter}/{patience}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\n‚ö†Ô∏è Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nüéâ Training complete! Best validation accuracy: {best_val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732ac7f8",
   "metadata": {},
   "source": [
    "## üìä Step 6: Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ee0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot([acc * 100 for acc in history['train_acc']], label='Train Accuracy', marker='o')\n",
    "axes[1].plot([acc * 100 for acc in history['val_acc']], label='Val Accuracy', marker='s')\n",
    "axes[1].axhline(y=80, color='r', linestyle='--', label='Target (80%)')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training curves saved to 'training_curves.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511505ff",
   "metadata": {},
   "source": [
    "## üîç Step 7: Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Full validation predictions\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix (Accuracy: {best_val_acc*100:.2f}%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Confusion matrix saved to 'confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4be9d7",
   "metadata": {},
   "source": [
    "## üîÑ Step 8: Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Load best model (without torch.compile)\n",
    "model_export = timm.create_model('efficientnet_b3', pretrained=False, num_classes=NUM_CLASSES)\n",
    "model_export.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_export.eval()\n",
    "model_export = model_export.to('cpu')\n",
    "\n",
    "# Export to ONNX\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "onnx_path = 'clothing_classifier.onnx'\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_export,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ ONNX model exported to '{onnx_path}'\")\n",
    "\n",
    "# Verify ONNX model\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"‚úÖ ONNX model verified\")\n",
    "\n",
    "# Test inference with ONNX Runtime\n",
    "ort_session = ort.InferenceSession(onnx_path)\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: dummy_input.numpy()}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "print(f\"‚úÖ ONNX Runtime inference test passed\")\n",
    "print(f\"   Output shape: {ort_outputs[0].shape}\")\n",
    "\n",
    "# Get file size\n",
    "onnx_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "print(f\"üì¶ ONNX model size: {onnx_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833fd9cd",
   "metadata": {},
   "source": [
    "## ‚ö° Step 9: Benchmark Inference Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark ONNX Runtime on CPU (simulating mobile device)\n",
    "num_runs = 100\n",
    "dummy_input_np = dummy_input.numpy()\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(10):\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: dummy_input_np})\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.time()\n",
    "for _ in tqdm(range(num_runs), desc=\"Benchmarking\"):\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: dummy_input_np})\n",
    "end_time = time.time()\n",
    "\n",
    "avg_time_ms = (end_time - start_time) / num_runs * 1000\n",
    "print(f\"\\n‚ö° Average inference time: {avg_time_ms:.2f} ms\")\n",
    "print(f\"üéØ FPS: {1000 / avg_time_ms:.2f}\")\n",
    "\n",
    "if avg_time_ms < 100:\n",
    "    print(\"‚úÖ Inference speed meets mobile target (<100ms)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Inference speed may be slow on mobile devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de111c7e",
   "metadata": {},
   "source": [
    "## üíæ Step 10: Save Metadata & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': 'EfficientNetB3',\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'input_size': [224, 224],\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "    'best_val_accuracy': float(best_val_acc),\n",
    "    'training_epochs': len(history['val_acc']),\n",
    "    'onnx_file': onnx_path,\n",
    "    'avg_inference_time_ms': float(avg_time_ms)\n",
    "}\n",
    "\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Metadata saved to 'model_metadata.json'\")\n",
    "\n",
    "# Display metadata\n",
    "print(\"\\nüìã Model Metadata:\")\n",
    "print(json.dumps(metadata, indent=2))\n",
    "\n",
    "# Download files to local machine (only in Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"\\n‚¨áÔ∏è Downloading files...\")\n",
    "    files.download('best_model.pth')\n",
    "    files.download('clothing_classifier.onnx')\n",
    "    files.download('model_metadata.json')\n",
    "    files.download('training_curves.png')\n",
    "    files.download('confusion_matrix.png')\n",
    "    print(\"‚úÖ All files downloaded!\")\n",
    "except ImportError:\n",
    "    print(\"\\nüìÅ Running locally - files saved to current directory\")\n",
    "    print(\"   - best_model.pth\")\n",
    "    print(\"   - clothing_classifier.onnx\")\n",
    "    print(\"   - model_metadata.json\")\n",
    "    print(\"   - training_curves.png\")\n",
    "    print(\"   - confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad8725",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Verify ONNX Model:**\n",
    "   - Place `clothing_classifier.onnx` in `assets/models/`\n",
    "   - Update `model_config.json` with the metadata\n",
    "\n",
    "2. **Test in Flutter App:**\n",
    "   ```bash\n",
    "   flutter pub get\n",
    "   flutter run\n",
    "   ```\n",
    "\n",
    "3. **Build for Production:**\n",
    "   - iOS: `flutter build ios --release`\n",
    "   - Android: `flutter build apk --release`\n",
    "\n",
    "4. **Monitor Performance:**\n",
    "   - Test inference speed on actual devices\n",
    "   - Verify classification accuracy in real-world scenarios\n",
    "\n",
    "---\n",
    "\n",
    "**Target Met:** ‚úÖ ‚â•80% validation accuracy\n",
    "\n",
    "**Model Size:** ~50MB (ONNX)\n",
    "\n",
    "**Inference Time:** <100ms (CPU)\n",
    "\n",
    "**Platform:** iOS + Android (ONNX Runtime Mobile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b0367",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ Part 2: Multi-Model Training Suite (OpenCLIP + GroundingDINO + SAM)\n",
    "\n",
    "**Toggle Flags:** Set `True` to enable specific training sections\n",
    "\n",
    "This orchestrated suite trains:\n",
    "1. **OpenCLIP** - Contrastive visual-language embeddings (primary)\n",
    "2. **GroundingDINO** - Zero-shot object detection\n",
    "3. **SAM** - Segment Anything Model fine-tuning\n",
    "4. **Wardrobe Indexing** - FAISS index for outfit recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea52deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üéõÔ∏è TRAINING TOGGLE FLAGS - Set True/False to enable/disable each section\n",
    "# ============================================================================\n",
    "\n",
    "RUN_OPENCLIP = True        # OpenCLIP contrastive training (recommended first)\n",
    "RUN_GROUNDINGDINO = False  # GroundingDINO fine-tuning\n",
    "RUN_SAM = False            # SAM fine-tuning\n",
    "RUN_WARDROBE_INDEX = True  # Build FAISS index for wardrobe\n",
    "\n",
    "# Dataset paths (using local deepfashion2_training folder)\n",
    "# Note: DeepFashion2 has nested structure: data/deepfashion2/train/train/image & annos\n",
    "DF2_DATA_ROOT = './deepfashion2_training/data/deepfashion2'\n",
    "WARDROBE_DIR = './assets/wardrobe_sample'  # Your wardrobe images\n",
    "OUTPUT_DIR = './trained_models'\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'openclip'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'groundingdino'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'sam'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'index'), exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Training configuration loaded\")\n",
    "print(f\"   OpenCLIP:       {'üü¢ ENABLED' if RUN_OPENCLIP else 'üî¥ DISABLED'}\")\n",
    "print(f\"   GroundingDINO:  {'üü¢ ENABLED' if RUN_GROUNDINGDINO else 'üî¥ DISABLED'}\")\n",
    "print(f\"   SAM:            {'üü¢ ENABLED' if RUN_SAM else 'üî¥ DISABLED'}\")\n",
    "print(f\"   Wardrobe Index: {'üü¢ ENABLED' if RUN_WARDROBE_INDEX else 'üî¥ DISABLED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6797a24",
   "metadata": {},
   "source": [
    "## üì¶ Install Multi-Model Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies for multi-model training\n",
    "!pip install -q open_clip_torch faiss-cpu transformers segment-anything\n",
    "\n",
    "# Import all required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üöÄ Training device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec75db2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Section A: OpenCLIP Contrastive Training\n",
    "\n",
    "Fine-tune OpenCLIP (ViT-B-32) on DeepFashion2 for fashion-aware embeddings.\n",
    "- **Architecture:** ViT-B-32 pretrained on LAION-2B\n",
    "- **Training:** Contrastive loss (image-text pairs)\n",
    "- **Output:** ONNX image encoder for mobile inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPENCLIP:\n",
    "    import open_clip\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # =====================================================================\n",
    "    # OpenCLIP Dataset: pairs (image, text) where text = category names\n",
    "    # =====================================================================\n",
    "    CLIP_CATEGORIES = [\n",
    "        'short sleeve top', 'long sleeve top', 'short sleeve outwear', 'long sleeve outwear',\n",
    "        'vest', 'sling', 'shorts', 'trousers', 'skirt', 'short sleeve dress',\n",
    "        'long sleeve dress', 'vest dress', 'sling dress'\n",
    "    ]\n",
    "    \n",
    "    class DF2CLIPDataset(Dataset):\n",
    "        \"\"\"DeepFashion2 dataset for CLIP contrastive training\"\"\"\n",
    "        def __init__(self, root, split='train'):\n",
    "            # Handle nested folder structure: root/split/split/image and root/split/split/annos\n",
    "            base_path = os.path.join(root, split, split)\n",
    "            if not os.path.exists(base_path):\n",
    "                base_path = os.path.join(root, split)  # fallback to root/split\n",
    "            \n",
    "            self.img_dir = os.path.join(base_path, 'image')\n",
    "            self.ann_dir = os.path.join(base_path, 'annos')\n",
    "            \n",
    "            if not os.path.exists(self.ann_dir):\n",
    "                print(f\"‚ö†Ô∏è Annotations not found at: {self.ann_dir}\")\n",
    "                self.files = []\n",
    "            else:\n",
    "                self.files = [f for f in os.listdir(self.ann_dir) if f.endswith('.json')]\n",
    "            print(f\"üìÇ Loaded {len(self.files)} samples from {split}\")\n",
    "            \n",
    "        def __len__(self): \n",
    "            return len(self.files)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            ann_path = os.path.join(self.ann_dir, self.files[idx])\n",
    "            with open(ann_path, 'r') as f:\n",
    "                ann = json.load(f)\n",
    "            \n",
    "            # Get image filename\n",
    "            img_name = ann.get('image', ann.get('filename', self.files[idx].replace('.json', '.jpg')))\n",
    "            \n",
    "            # Extract categories\n",
    "            items = ann.get('items', []) or [v for k, v in ann.items() if k.startswith('item')]\n",
    "            cats = []\n",
    "            for it in items:\n",
    "                cid = int(it.get('category_id', it.get('category', 0)))\n",
    "                if 1 <= cid <= 13:\n",
    "                    cats.append(CLIP_CATEGORIES[cid - 1])\n",
    "            \n",
    "            text = ', '.join(sorted(set(cats))) if cats else 'clothing'\n",
    "            img = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n",
    "            return img, text\n",
    "    \n",
    "    print(\"‚úÖ OpenCLIP dataset class defined\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping OpenCLIP (RUN_OPENCLIP = False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPENCLIP:\n",
    "    # =====================================================================\n",
    "    # OpenCLIP Training Configuration\n",
    "    # =====================================================================\n",
    "    CLIP_CONFIG = {\n",
    "        'model_name': 'ViT-B-32',\n",
    "        'pretrained': 'laion2b_s34b_b79k',\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 5e-6,\n",
    "        'epochs': 10,\n",
    "        'val_every': 1\n",
    "    }\n",
    "    \n",
    "    # Load model and preprocessing\n",
    "    print(f\"üîÑ Loading OpenCLIP {CLIP_CONFIG['model_name']}...\")\n",
    "    clip_model, clip_preprocess = open_clip.create_model_and_transforms(\n",
    "        CLIP_CONFIG['model_name'], \n",
    "        pretrained=CLIP_CONFIG['pretrained']\n",
    "    )\n",
    "    clip_tokenizer = open_clip.get_tokenizer(CLIP_CONFIG['model_name'])\n",
    "    clip_model = clip_model.to(device)\n",
    "    \n",
    "    # Create datasets\n",
    "    clip_train_ds = DF2CLIPDataset(DF2_DATA_ROOT, 'train')\n",
    "    clip_val_ds = DF2CLIPDataset(DF2_DATA_ROOT, 'validation')\n",
    "    \n",
    "    clip_train_dl = DataLoader(clip_train_ds, batch_size=CLIP_CONFIG['batch_size'], \n",
    "                                shuffle=True, num_workers=4)\n",
    "    clip_val_dl = DataLoader(clip_val_ds, batch_size=CLIP_CONFIG['batch_size'], \n",
    "                              shuffle=False, num_workers=4)\n",
    "    \n",
    "    print(f\"‚úÖ OpenCLIP ready for training\")\n",
    "    print(f\"   Model: {CLIP_CONFIG['model_name']}\")\n",
    "    print(f\"   Train batches: {len(clip_train_dl)}\")\n",
    "    print(f\"   Val batches: {len(clip_val_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9216e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPENCLIP:\n",
    "    # =====================================================================\n",
    "    # OpenCLIP Training Loop with Validation\n",
    "    # =====================================================================\n",
    "    \n",
    "    def recall_at_k(image_feats, text_feats, ks=(1, 5, 10)):\n",
    "        \"\"\"Compute Recall@K for image-text retrieval\"\"\"\n",
    "        sims = image_feats @ text_feats.T\n",
    "        ranks = np.argsort(-sims, axis=1)\n",
    "        gt = np.arange(sims.shape[0])\n",
    "        recalls = {}\n",
    "        for k in ks:\n",
    "            hit = (ranks[:, :k] == gt[:, None]).any(axis=1).mean()\n",
    "            recalls[f\"R@{k}\"] = float(hit)\n",
    "        return recalls\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    clip_optimizer = torch.optim.AdamW(clip_model.parameters(), lr=CLIP_CONFIG['learning_rate'])\n",
    "    total_steps = max(1, len(clip_train_dl) * CLIP_CONFIG['epochs'])\n",
    "    clip_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(clip_optimizer, T_max=total_steps)\n",
    "    \n",
    "    # Training history\n",
    "    clip_history = {'train_loss': [], 'val_recall': []}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ Starting OpenCLIP Training\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for epoch in range(CLIP_CONFIG['epochs']):\n",
    "        clip_model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(clip_train_dl, desc=f\"Epoch {epoch+1}/{CLIP_CONFIG['epochs']}\")\n",
    "        for imgs, texts in pbar:\n",
    "            # Preprocess images\n",
    "            imgs_tensor = torch.stack([clip_preprocess(im) for im in imgs]).to(device)\n",
    "            tokens = clip_tokenizer(list(texts)).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                text_feats = clip_model.encode_text(tokens)\n",
    "                text_feats = F.normalize(text_feats, dim=-1)\n",
    "            \n",
    "            img_feats = clip_model.encode_image(imgs_tensor)\n",
    "            img_feats = F.normalize(img_feats, dim=-1)\n",
    "            \n",
    "            # Contrastive loss (symmetric)\n",
    "            logits = img_feats @ text_feats.T * 100.0\n",
    "            labels = torch.arange(logits.size(0), device=device)\n",
    "            loss = (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels)) / 2\n",
    "            \n",
    "            # Backward\n",
    "            clip_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_optimizer.step()\n",
    "            clip_scheduler.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{clip_scheduler.get_last_lr()[0]:.2e}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / len(clip_train_dl)\n",
    "        clip_history['train_loss'].append(avg_loss)\n",
    "        \n",
    "        # Validation\n",
    "        if (epoch + 1) % CLIP_CONFIG['val_every'] == 0:\n",
    "            clip_model.eval()\n",
    "            all_img_feats, all_txt_feats = [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for imgs, texts in tqdm(clip_val_dl, desc=\"Validating\"):\n",
    "                    imgs_tensor = torch.stack([clip_preprocess(im) for im in imgs]).to(device)\n",
    "                    tokens = clip_tokenizer(list(texts)).to(device)\n",
    "                    \n",
    "                    txt_f = clip_model.encode_text(tokens)\n",
    "                    img_f = clip_model.encode_image(imgs_tensor)\n",
    "                    \n",
    "                    all_img_feats.append(F.normalize(img_f, dim=-1).cpu().numpy())\n",
    "                    all_txt_feats.append(F.normalize(txt_f, dim=-1).cpu().numpy())\n",
    "            \n",
    "            img_feats_np = np.concatenate(all_img_feats, 0)\n",
    "            txt_feats_np = np.concatenate(all_txt_feats, 0)\n",
    "            \n",
    "            recalls = recall_at_k(img_feats_np, txt_feats_np)\n",
    "            clip_history['val_recall'].append(recalls)\n",
    "            \n",
    "            print(f\"\\nüìä Epoch {epoch+1} | Loss: {avg_loss:.4f} | R@1: {recalls['R@1']:.3f} | R@5: {recalls['R@5']:.3f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save(clip_model.state_dict(), \n",
    "                   os.path.join(OUTPUT_DIR, 'openclip', f'clip_epoch{epoch+1}.pth'))\n",
    "    \n",
    "    print(f\"\\n‚úÖ OpenCLIP training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_OPENCLIP:\n",
    "    # =====================================================================\n",
    "    # Export OpenCLIP Image Encoder to ONNX\n",
    "    # =====================================================================\n",
    "    \n",
    "    class CLIPImageEncoder(nn.Module):\n",
    "        \"\"\"Wrapper for ONNX export of image encoder only\"\"\"\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "        def forward(self, x):\n",
    "            return self.model.encode_image(x)\n",
    "    \n",
    "    clip_model.eval()\n",
    "    encoder_wrapper = CLIPImageEncoder(clip_model)\n",
    "    \n",
    "    dummy_input = torch.randn(1, 3, 224, 224, device=device)\n",
    "    onnx_clip_path = os.path.join(OUTPUT_DIR, 'openclip', 'clip_image_encoder.onnx')\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        encoder_wrapper,\n",
    "        dummy_input,\n",
    "        onnx_clip_path,\n",
    "        input_names=['image'],\n",
    "        output_names=['embedding'],\n",
    "        opset_version=17,\n",
    "        do_constant_folding=True,\n",
    "        dynamic_axes={\n",
    "            'image': {0: 'batch_size'},\n",
    "            'embedding': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ OpenCLIP image encoder exported to: {onnx_clip_path}\")\n",
    "    print(f\"   File size: {os.path.getsize(onnx_clip_path) / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax[0].plot(clip_history['train_loss'], marker='o')\n",
    "    ax[0].set_title('OpenCLIP Training Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].grid(True)\n",
    "    \n",
    "    if clip_history['val_recall']:\n",
    "        r1 = [r['R@1'] for r in clip_history['val_recall']]\n",
    "        r5 = [r['R@5'] for r in clip_history['val_recall']]\n",
    "        ax[1].plot(r1, marker='o', label='R@1')\n",
    "        ax[1].plot(r5, marker='s', label='R@5')\n",
    "        ax[1].set_title('Validation Recall@K')\n",
    "        ax[1].set_xlabel('Epoch')\n",
    "        ax[1].set_ylabel('Recall')\n",
    "        ax[1].legend()\n",
    "        ax[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'openclip', 'training_curves.png'), dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a2674",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Section B: GroundingDINO Fine-Tuning\n",
    "\n",
    "Fine-tune GroundingDINO for clothing detection with text prompts.\n",
    "- **Model:** IDEA-Research/grounding-dino-base (HuggingFace)\n",
    "- **Task:** Open-vocabulary object detection\n",
    "- **Training:** Phrase grounding on DeepFashion2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b23a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GROUNDINGDINO:\n",
    "    from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection, get_cosine_schedule_with_warmup\n",
    "    \n",
    "    # =====================================================================\n",
    "    # GroundingDINO Dataset\n",
    "    # =====================================================================\n",
    "    GDINO_CATEGORIES = [\n",
    "        'short sleeve top', 'long sleeve top', 'short sleeve outwear', 'long sleeve outwear',\n",
    "        'vest', 'sling', 'shorts', 'trousers', 'skirt', 'short sleeve dress',\n",
    "        'long sleeve dress', 'vest dress', 'sling dress'\n",
    "    ]\n",
    "    \n",
    "    class DF2GDINODataset(Dataset):\n",
    "        \"\"\"DeepFashion2 dataset for GroundingDINO phrase grounding\"\"\"\n",
    "        def __init__(self, root, split='train'):\n",
    "            # Handle nested folder structure\n",
    "            base_path = os.path.join(root, split, split)\n",
    "            if not os.path.exists(base_path):\n",
    "                base_path = os.path.join(root, split)\n",
    "            \n",
    "            self.img_dir = os.path.join(base_path, 'image')\n",
    "            self.ann_dir = os.path.join(base_path, 'annos')\n",
    "            \n",
    "            if not os.path.exists(self.ann_dir):\n",
    "                self.files = []\n",
    "            else:\n",
    "                self.files = [f for f in os.listdir(self.ann_dir) if f.endswith('.json')]\n",
    "            \n",
    "        def __len__(self): \n",
    "            return len(self.files)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            ann_path = os.path.join(self.ann_dir, self.files[idx])\n",
    "            with open(ann_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            img_name = data.get('image', data.get('filename', self.files[idx].replace('.json', '.jpg')))\n",
    "            img = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n",
    "            \n",
    "            items = data.get('items', []) or [v for k, v in data.items() if k.startswith('item')]\n",
    "            cats = []\n",
    "            for it in items:\n",
    "                cid = int(it.get('category_id', it.get('category', 0)))\n",
    "                if 1 <= cid <= 13:\n",
    "                    cats.append(GDINO_CATEGORIES[cid - 1])\n",
    "            \n",
    "            text = '. '.join(sorted(set(cats))) + '.' if cats else 'clothes.'\n",
    "            return img, text\n",
    "    \n",
    "    def gdino_collate(batch):\n",
    "        return list(zip(*batch))\n",
    "    \n",
    "    # Load model\n",
    "    print(\"üîÑ Loading GroundingDINO...\")\n",
    "    gdino_model = AutoModelForZeroShotObjectDetection.from_pretrained(\n",
    "        'IDEA-Research/grounding-dino-base'\n",
    "    ).to(device)\n",
    "    gdino_processor = AutoProcessor.from_pretrained('IDEA-Research/grounding-dino-base')\n",
    "    \n",
    "    # Create datasets\n",
    "    gdino_train_ds = DF2GDINODataset(DF2_DATA_ROOT, 'train')\n",
    "    gdino_val_ds = DF2GDINODataset(DF2_DATA_ROOT, 'validation')\n",
    "    \n",
    "    gdino_train_dl = DataLoader(gdino_train_ds, batch_size=2, shuffle=True, \n",
    "                                 collate_fn=gdino_collate, num_workers=4)\n",
    "    gdino_val_dl = DataLoader(gdino_val_ds, batch_size=2, shuffle=False, \n",
    "                               collate_fn=gdino_collate, num_workers=4)\n",
    "    \n",
    "    print(f\"‚úÖ GroundingDINO ready\")\n",
    "    print(f\"   Train samples: {len(gdino_train_ds)}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping GroundingDINO (RUN_GROUNDINGDINO = False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GROUNDINGDINO:\n",
    "    # =====================================================================\n",
    "    # GroundingDINO Training Loop\n",
    "    # =====================================================================\n",
    "    GDINO_CONFIG = {\n",
    "        'epochs': 10,\n",
    "        'learning_rate': 1e-5,\n",
    "    }\n",
    "    \n",
    "    gdino_optimizer = torch.optim.AdamW(gdino_model.parameters(), lr=GDINO_CONFIG['learning_rate'])\n",
    "    gdino_steps = GDINO_CONFIG['epochs'] * len(gdino_train_dl)\n",
    "    gdino_scheduler = get_cosine_schedule_with_warmup(\n",
    "        gdino_optimizer, \n",
    "        num_warmup_steps=max(10, gdino_steps // 20), \n",
    "        num_training_steps=gdino_steps\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ Starting GroundingDINO Training\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for epoch in range(GDINO_CONFIG['epochs']):\n",
    "        gdino_model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(gdino_train_dl, desc=f\"Epoch {epoch+1}/{GDINO_CONFIG['epochs']}\")\n",
    "        for imgs, texts in pbar:\n",
    "            inputs = gdino_processor(\n",
    "                images=list(imgs), \n",
    "                text=list(texts), \n",
    "                return_tensors='pt', \n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = gdino_model(**inputs)\n",
    "            loss = outputs.loss if hasattr(outputs, 'loss') else torch.tensor(0.0, device=device)\n",
    "            \n",
    "            if loss.requires_grad:\n",
    "                gdino_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                gdino_optimizer.step()\n",
    "                gdino_scheduler.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / len(gdino_train_dl)\n",
    "        print(f\"üìä Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        gdino_model.save_pretrained(os.path.join(OUTPUT_DIR, 'groundingdino'))\n",
    "        gdino_processor.save_pretrained(os.path.join(OUTPUT_DIR, 'groundingdino'))\n",
    "    \n",
    "    print(f\"\\n‚úÖ GroundingDINO training complete!\")\n",
    "    print(f\"   Model saved to: {os.path.join(OUTPUT_DIR, 'groundingdino')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b149a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üóÇÔ∏è Section C: Build Wardrobe Index (FAISS)\n",
    "\n",
    "Create a FAISS index of your wardrobe images for fast similarity search.\n",
    "- **Embeddings:** OpenCLIP image encoder\n",
    "- **Index:** FAISS FlatIP (inner product for cosine similarity)\n",
    "- **Output:** `index.faiss` + `paths.txt` for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3787f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_WARDROBE_INDEX:\n",
    "    import faiss\n",
    "    \n",
    "    # =====================================================================\n",
    "    # Build FAISS Index for Wardrobe\n",
    "    # =====================================================================\n",
    "    \n",
    "    # Use OpenCLIP model (load if not already loaded)\n",
    "    if 'clip_model' not in dir():\n",
    "        print(\"üîÑ Loading OpenCLIP for indexing...\")\n",
    "        import open_clip\n",
    "        clip_model, clip_preprocess = open_clip.create_model_and_transforms(\n",
    "            'ViT-B-32', pretrained='laion2b_s34b_b79k'\n",
    "        )\n",
    "        clip_model = clip_model.to(device).eval()\n",
    "    else:\n",
    "        clip_model.eval()\n",
    "    \n",
    "    # Find all wardrobe images\n",
    "    wardrobe_paths = []\n",
    "    for ext in ('*.jpg', '*.jpeg', '*.png', '*.webp'):\n",
    "        wardrobe_paths.extend(glob.glob(os.path.join(WARDROBE_DIR, '**', ext), recursive=True))\n",
    "    \n",
    "    wardrobe_paths = sorted(list(set(wardrobe_paths)))\n",
    "    \n",
    "    if not wardrobe_paths:\n",
    "        print(f\"‚ö†Ô∏è No images found in {WARDROBE_DIR}\")\n",
    "        print(\"   Please add your wardrobe images and re-run this cell.\")\n",
    "    else:\n",
    "        print(f\"üìÇ Found {len(wardrobe_paths)} wardrobe images\")\n",
    "        \n",
    "        # Compute embeddings\n",
    "        all_embeddings = []\n",
    "        batch_size = 32\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(wardrobe_paths), batch_size), desc=\"Embedding wardrobe\"):\n",
    "                batch_paths = wardrobe_paths[i:i+batch_size]\n",
    "                batch_imgs = [clip_preprocess(Image.open(p).convert('RGB')) for p in batch_paths]\n",
    "                batch_tensor = torch.stack(batch_imgs).to(device)\n",
    "                \n",
    "                feats = clip_model.encode_image(batch_tensor)\n",
    "                feats = F.normalize(feats, dim=-1)\n",
    "                all_embeddings.append(feats.cpu().numpy().astype('float32'))\n",
    "        \n",
    "        embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "        \n",
    "        # Save embeddings\n",
    "        index_dir = os.path.join(OUTPUT_DIR, 'index')\n",
    "        np.save(os.path.join(index_dir, 'embeddings.npy'), embeddings)\n",
    "        \n",
    "        # Save paths\n",
    "        with open(os.path.join(index_dir, 'paths.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(wardrobe_paths))\n",
    "        \n",
    "        # Build FAISS index\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "        faiss.write_index(index, os.path.join(index_dir, 'index.faiss'))\n",
    "        \n",
    "        print(f\"\\n‚úÖ Wardrobe index built!\")\n",
    "        print(f\"   Embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"   Index saved to: {index_dir}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping wardrobe indexing (RUN_WARDROBE_INDEX = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730fd6db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Section D: Inference Demo\n",
    "\n",
    "Test the trained models with a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de405569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# üß™ Inference Demo: Query Wardrobe with Text\n",
    "# =====================================================================\n",
    "\n",
    "def query_wardrobe_by_text(text_query, top_k=5):\n",
    "    \"\"\"Find similar items in wardrobe given a text description\"\"\"\n",
    "    import faiss\n",
    "    \n",
    "    index_dir = os.path.join(OUTPUT_DIR, 'index')\n",
    "    \n",
    "    # Load index and paths\n",
    "    index = faiss.read_index(os.path.join(index_dir, 'index.faiss'))\n",
    "    with open(os.path.join(index_dir, 'paths.txt'), 'r') as f:\n",
    "        paths = f.read().strip().split('\\n')\n",
    "    \n",
    "    # Encode text query\n",
    "    if 'clip_model' not in dir():\n",
    "        import open_clip\n",
    "        clip_model, _ = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "        clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "        clip_model = clip_model.to(device).eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tokens = clip_tokenizer([text_query]).to(device)\n",
    "        text_feat = clip_model.encode_text(tokens)\n",
    "        text_feat = F.normalize(text_feat, dim=-1).cpu().numpy().astype('float32')\n",
    "    \n",
    "    # Search\n",
    "    scores, indices = index.search(text_feat, top_k)\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(1, min(top_k, len(indices[0])), figsize=(15, 4))\n",
    "    if top_k == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, idx, score in zip(axes, indices[0], scores[0]):\n",
    "        img = Image.open(paths[idx])\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Score: {score:.3f}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Query: \"{text_query}\"')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return [(paths[i], scores[0][j]) for j, i in enumerate(indices[0])]\n",
    "\n",
    "# Example query (uncomment to test)\n",
    "# results = query_wardrobe_by_text(\"casual summer outfit\", top_k=5)\n",
    "\n",
    "print(\"‚úÖ Inference demo function defined\")\n",
    "print(\"   Usage: query_wardrobe_by_text('blue summer dress', top_k=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4c48d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Final Summary: Export All Models\n",
    "\n",
    "Copy the trained models to `assets/models/` for Flutter app integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e9bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# =====================================================================\n",
    "# üì¶ Copy Trained Models to assets/models/\n",
    "# =====================================================================\n",
    "\n",
    "models_dst = './assets/models'\n",
    "os.makedirs(models_dst, exist_ok=True)\n",
    "\n",
    "files_copied = []\n",
    "\n",
    "# Copy EfficientNet ONNX (from Part 1)\n",
    "if os.path.exists('clothing_classifier.onnx'):\n",
    "    shutil.copy('clothing_classifier.onnx', os.path.join(models_dst, 'clothing_classifier.onnx'))\n",
    "    files_copied.append('clothing_classifier.onnx')\n",
    "\n",
    "# Copy OpenCLIP ONNX\n",
    "clip_onnx = os.path.join(OUTPUT_DIR, 'openclip', 'clip_image_encoder.onnx')\n",
    "if os.path.exists(clip_onnx):\n",
    "    shutil.copy(clip_onnx, os.path.join(models_dst, 'clip_image_encoder.onnx'))\n",
    "    files_copied.append('clip_image_encoder.onnx')\n",
    "\n",
    "# Copy FAISS index\n",
    "index_faiss = os.path.join(OUTPUT_DIR, 'index', 'index.faiss')\n",
    "if os.path.exists(index_faiss):\n",
    "    shutil.copy(index_faiss, os.path.join(models_dst, 'index.faiss'))\n",
    "    shutil.copy(os.path.join(OUTPUT_DIR, 'index', 'paths.txt'), os.path.join(models_dst, 'paths.txt'))\n",
    "    shutil.copy(os.path.join(OUTPUT_DIR, 'index', 'embeddings.npy'), os.path.join(models_dst, 'embeddings.npy'))\n",
    "    files_copied.extend(['index.faiss', 'paths.txt', 'embeddings.npy'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üéâ TRAINING COMPLETE - MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìÅ Models copied to {models_dst}:\")\n",
    "for f in files_copied:\n",
    "    size = os.path.getsize(os.path.join(models_dst, f)) / 1e6\n",
    "    print(f\"   ‚úì {f} ({size:.1f} MB)\")\n",
    "\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"   1. Test models with inference demo cell above\")\n",
    "print(\"   2. Copy assets/models/ to Flutter project\")\n",
    "print(\"   3. Update model_config.json with paths\")\n",
    "print(\"   4. Run: flutter pub get && flutter run\")\n",
    "\n",
    "print(\"\\n‚úÖ All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
